{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4e913-aa77-4efa-bbb4-46413f505bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import findspark\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# -----------------------------\n",
    "# Config (paths relative to root)\n",
    "# -----------------------------\n",
    "INPUT_FILE = \"project/data/unstructured/unstructured_students_data.jsonl\"\n",
    "OUTPUT_DIR = \"project/data/bronze_output\"\n",
    "OUTPUT_FILE_FINAL = os.path.join(OUTPUT_DIR, \"bronze_students.jsonl\")\n",
    "\n",
    "# -----------------------------\n",
    "# Delete output directory first (if exists)\n",
    "# -----------------------------\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize Spark\n",
    "# -----------------------------\n",
    "findspark.init()\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col\n",
    "    from pyspark.sql.types import *\n",
    "    pyspark_available = True\n",
    "except ImportError:\n",
    "    print(\"PySpark not available. Install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "if pyspark_available:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Student Dataset Bronze Layer\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"Spark session initialized successfully!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "else:\n",
    "    raise SystemExit(\"PySpark not available, exiting.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Read raw JSONL as RDD\n",
    "# -----------------------------\n",
    "print(f\"Reading from: {INPUT_FILE}\")\n",
    "raw_rdd = spark.sparkContext.textFile(INPUT_FILE)\n",
    "print(f\"Raw RDD partition count: {raw_rdd.getNumPartitions()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Parsing function - PURE RDD APPROACH\n",
    "# -----------------------------\n",
    "def create_bronze_record(line):\n",
    "    \"\"\"Process each line and return JSON string directly - no DataFrame conversion issues\"\"\"\n",
    "    try:\n",
    "        data = json.loads(line)\n",
    "        data['_ingestion_timestamp'] = time.time()\n",
    "        data['_source'] = 'jsonl_file'\n",
    "        data['_status'] = 'valid'\n",
    "        return json.dumps(data)\n",
    "    except Exception as e:\n",
    "        error_record = {\n",
    "            '_raw_data': line,\n",
    "            '_ingestion_timestamp': time.time(),\n",
    "            '_source': 'jsonl_file',\n",
    "            '_status': 'parse_error',\n",
    "            '_error_message': str(e)\n",
    "        }\n",
    "        return json.dumps(error_record)\n",
    "\n",
    "# -----------------------------\n",
    "# Apply parsing and get quality metrics\n",
    "# -----------------------------\n",
    "# Process the RDD to get JSON strings\n",
    "bronze_json_rdd = raw_rdd.map(create_bronze_record)\n",
    "\n",
    "# Count total records\n",
    "total_records = raw_rdd.count()\n",
    "\n",
    "# Count valid vs error records by checking the JSON content\n",
    "def count_records_status(json_str):\n",
    "    \"\"\"Helper function to count valid vs error records\"\"\"\n",
    "    record = json.loads(json_str)\n",
    "    return (1, 0) if record.get('_status') == 'valid' else (0, 1)\n",
    "\n",
    "status_counts = bronze_json_rdd.map(count_records_status).reduce(lambda a, b: (a[0] + b[0], a[1] + b[1]))\n",
    "valid_records, error_records = status_counts\n",
    "\n",
    "print(\"\\n=== Bronze Layer Data Quality ===\")\n",
    "print(f\"Total records: {total_records}\")\n",
    "print(f\"Valid records: {valid_records}\")\n",
    "print(f\"Parse errors: {error_records}\")\n",
    "print(f\"Success rate: {(valid_records / total_records) * 100:.1f}%\")\n",
    "\n",
    "# Show some examples of parse errors\n",
    "if error_records > 0:\n",
    "    print(\"\\n=== Parse Error Examples ===\")\n",
    "    # Get some error records to show\n",
    "    error_samples = bronze_json_rdd.filter(lambda json_str: json.loads(json_str).get('_status') == 'parse_error').take(3)\n",
    "    for i, error_json in enumerate(error_samples):\n",
    "        error_data = json.loads(error_json)\n",
    "        print(f\"Error example {i}:\")\n",
    "        print(f\"  Raw data: {error_data.get('_raw_data', '')[:100]}...\")  # Show first 100 chars\n",
    "        print(f\"  Error: {error_data.get('_error_message', '')}\")\n",
    "        print()\n",
    "\n",
    "# -----------------------------\n",
    "# Save Bronze layer as JSONL\n",
    "# -----------------------------\n",
    "print(\"Saving bronze layer...\")\n",
    "bronze_json_rdd.coalesce(1).saveAsTextFile(OUTPUT_DIR)\n",
    "\n",
    "# Rename part file to final filename\n",
    "part_files = glob.glob(os.path.join(OUTPUT_DIR, 'part-*'))\n",
    "if part_files:\n",
    "    part_file = part_files[0]\n",
    "    os.rename(part_file, OUTPUT_FILE_FINAL)\n",
    "    print(f\"✓ Renamed {part_file} to {OUTPUT_FILE_FINAL}\")\n",
    "    \n",
    "    # Clean up Spark output directory\n",
    "    success_file = os.path.join(OUTPUT_DIR, '_SUCCESS')\n",
    "    if os.path.exists(success_file):\n",
    "        os.remove(success_file)\n",
    "    if os.path.exists(OUTPUT_DIR) and not os.listdir(OUTPUT_DIR):\n",
    "        os.rmdir(OUTPUT_DIR)\n",
    "    \n",
    "    print(f\"✓ Bronze layer saved to: {OUTPUT_FILE_FINAL}\")\n",
    "    \n",
    "    # Verify output\n",
    "    if os.path.exists(OUTPUT_FILE_FINAL):\n",
    "        file_size = os.path.getsize(OUTPUT_FILE_FINAL)\n",
    "        line_count = sum(1 for _ in open(OUTPUT_FILE_FINAL))\n",
    "        print(f\"✓ Output file: {file_size} bytes, {line_count} lines\")\n",
    "        \n",
    "        # Show first few lines\n",
    "        print(\"\\n=== First 3 lines of output ===\")\n",
    "        with open(OUTPUT_FILE_FINAL, 'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i < 3:\n",
    "                    # Parse and pretty print for readability\n",
    "                    try:\n",
    "                        record = json.loads(line.strip())\n",
    "                        print(f\"Line {i}: {json.dumps(record, indent=2)[:200]}...\")\n",
    "                    except:\n",
    "                        print(f\"Line {i}: {line.strip()[:100]}...\")\n",
    "                else:\n",
    "                    break\n",
    "else:\n",
    "    print(\"❌ No output files were created!\")\n",
    "\n",
    "# -----------------------------\n",
    "# Stop Spark\n",
    "# -----------------------------\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9d2b9-bb3a-4a19-8522-240295dc5c19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
