{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c2edc5e",
   "metadata": {},
   "source": [
    "# Lab Exercise 2: Introduction to PySpark and Data Inspection\n",
    "\n",
    "### Learning Objectives\n",
    "- Set up and run first PySpark applications\n",
    "- Understand Spark DataFrames and basic operations\n",
    "- Learn data inspection techniques essential for Medallion Architecture\n",
    "- Practice with different file formats (CSV, JSON, Parquet)\n",
    "\n",
    "### Lab Overview\n",
    "This lab introduces PySpark fundamentals and data inspection techniques that you'll use throughout your data pipeline projects.\n",
    "\n",
    "### Timeline\n",
    "- **Part 1**: PySpark Environment Setup (15-20 minutes)\n",
    "- **Part 2**: Data Loading and Schema Exploration (30-40 minutes)\n",
    "- **Part 3**: DataFrame Operations (40-50 minutes)\n",
    "- **Part 4**: Introduction to Spark SQL (20-30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e94541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# PySpark imports\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    pyspark_available = True\n",
    "    print(\"PySpark is available!\")\n",
    "except ImportError:\n",
    "    print(\"PySpark not found. Please install with: pip install pyspark\")\n",
    "    pyspark_available = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84eca89",
   "metadata": {},
   "source": [
    "## Part 1: PySpark Environment Setup\n",
    "\n",
    "Let's start by setting up our Spark environment and creating our first SparkSession."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aea79c",
   "metadata": {},
   "source": [
    "### Task 1.1: Create and Configure SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb1921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    # Create SparkSession with custom configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Lab2-PySpark-Basics\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce verbose output\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    print(\"✓ SparkSession created successfully!\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "    print(f\"Master: {spark.sparkContext.master}\")\n",
    "    \n",
    "    # Check available cores and memory\n",
    "    print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot proceed without PySpark. Please install PySpark first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4110078a",
   "metadata": {},
   "source": [
    "### Task 1.2: Environment Verification\n",
    "\n",
    "Let's verify our Spark environment is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d9d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    # Test basic Spark functionality\n",
    "    print(\"=== Testing Spark Functionality ===\")\n",
    "    \n",
    "    # Create a simple RDD to test\n",
    "    test_data = [1, 2, 3, 4, 5]\n",
    "    test_rdd = spark.sparkContext.parallelize(test_data)\n",
    "    \n",
    "    # Basic operations\n",
    "    sum_result = test_rdd.reduce(lambda a, b: a + b)\n",
    "    count_result = test_rdd.count()\n",
    "    \n",
    "    print(f\"Test data sum: {sum_result}\")\n",
    "    print(f\"Test data count: {count_result}\")\n",
    "    \n",
    "    # Create a simple DataFrame\n",
    "    test_df = spark.createDataFrame([(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\")], [\"id\", \"name\"])\n",
    "    \n",
    "    print(\"\\nTest DataFrame:\")\n",
    "    test_df.show()\n",
    "    \n",
    "    print(\"✓ Spark environment is working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c302e",
   "metadata": {},
   "source": [
    "## Part 2: Data Loading and Schema Exploration\n",
    "\n",
    "Now let's work with realistic datasets that represent typical enterprise data scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a0300",
   "metadata": {},
   "source": [
    "### Task 2.1: Create Sample Datasets\n",
    "\n",
    "First, let's create some sample datasets that simulate real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample datasets for the lab\n",
    "import tempfile\n",
    "import builtins\n",
    "import shutil\n",
    "\n",
    "# Create a temporary directory for our sample data\n",
    "temp_dir = tempfile.mkdtemp(prefix=\"lab2_data_\")\n",
    "print(f\"Sample data directory: {temp_dir}\")\n",
    "\n",
    "# 1. E-commerce Customer Data (CSV)\n",
    "customers_data = []\n",
    "for i in range(1, 101):\n",
    "    customer = {\n",
    "        'customer_id': i,\n",
    "        'first_name': random.choice(['John', 'Jane', 'Bob', 'Alice', 'Charlie', 'Diana', 'Eve', 'Frank']),\n",
    "        'last_name': random.choice(['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis']),\n",
    "        'email': f'customer{i}@email.com',\n",
    "        'age': random.randint(18, 80),\n",
    "        'city': random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia']),\n",
    "        'signup_date': (datetime.now() - timedelta(days=random.randint(1, 365))).strftime('%Y-%m-%d'),\n",
    "        'total_orders': random.randint(0, 50),\n",
    "        'total_spent': builtins.round(random.uniform(0, 5000), 2)\n",
    "    }\n",
    "    customers_data.append(customer)\n",
    "\n",
    "# Save as CSV\n",
    "customers_df = pd.DataFrame(customers_data)\n",
    "customers_csv_path = os.path.join(temp_dir, \"customers.csv\")\n",
    "customers_df.to_csv(customers_csv_path, index=False)\n",
    "\n",
    "print(f\"✓ Created customers.csv with {len(customers_data)} records\")\n",
    "\n",
    "# 2. Transaction Logs (JSON)\n",
    "transactions_data = []\n",
    "for i in range(1, 501):\n",
    "    transaction = {\n",
    "        'transaction_id': f'TXN_{i:05d}',\n",
    "        'customer_id': random.randint(1, 100),\n",
    "        'product_id': f'PROD_{random.randint(1, 50):03d}',\n",
    "        'product_category': random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports']),\n",
    "        'amount': builtins.round(random.uniform(10, 500), 2),\n",
    "        'timestamp': (datetime.now() - timedelta(hours=random.randint(1, 720))).isoformat(),\n",
    "        'payment_method': random.choice(['credit_card', 'debit_card', 'paypal', 'cash']),\n",
    "        'status': random.choice(['completed', 'pending', 'cancelled']),\n",
    "        'metadata': {\n",
    "            'device': random.choice(['mobile', 'desktop', 'tablet']),\n",
    "            'browser': random.choice(['chrome', 'firefox', 'safari', 'edge']),\n",
    "            'location': random.choice(['US', 'CA', 'UK', 'DE', 'FR'])\n",
    "        }\n",
    "    }\n",
    "    transactions_data.append(transaction)\n",
    "\n",
    "# Save as JSON\n",
    "transactions_json_path = os.path.join(temp_dir, \"transactions.json\")\n",
    "with open(transactions_json_path, 'w') as f:\n",
    "    for transaction in transactions_data:\n",
    "        f.write(json.dumps(transaction) + '\\n')\n",
    "\n",
    "print(f\"✓ Created transactions.json with {len(transactions_data)} records\")\n",
    "\n",
    "# 3. Product Catalog (will be saved as Parquet later)\n",
    "products_data = []\n",
    "for i in range(1, 51):\n",
    "    product = {\n",
    "        'product_id': f'PROD_{i:03d}',\n",
    "        'product_name': f'Product {i}',\n",
    "        'category': random.choice(['Electronics', 'Clothing', 'Books', 'Home', 'Sports']),\n",
    "        'price': builtins.round(random.uniform(10, 1000), 2),\n",
    "        'stock_quantity': random.randint(0, 1000),\n",
    "        'supplier': random.choice(['Supplier A', 'Supplier B', 'Supplier C', 'Supplier D']),\n",
    "        'weight_kg': builtins.round(random.uniform(0.1, 50), 2),\n",
    "        'created_date': (datetime.now() - timedelta(days=random.randint(30, 1000))).strftime('%Y-%m-%d'),\n",
    "        'is_active': random.choice([True, False])\n",
    "    }\n",
    "    products_data.append(product)\n",
    "\n",
    "print(f\"✓ Created product data with {len(products_data)} records\")\n",
    "print(f\"Sample data files ready in: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d1716",
   "metadata": {},
   "source": [
    "### Task 2.2: Loading CSV Data\n",
    "\n",
    "Let's start by loading and exploring the customer CSV data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b260a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Loading CSV Data ===\")\n",
    "    \n",
    "    # Load CSV with header and automatic type inference\n",
    "    customers_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(customers_csv_path)\n",
    "    \n",
    "    print(\"✓ CSV loaded successfully!\")\n",
    "    \n",
    "    # Basic information about the DataFrame\n",
    "    print(f\"Number of rows: {customers_df.count()}\")\n",
    "    print(f\"Number of columns: {len(customers_df.columns)}\")\n",
    "    print(f\"Columns: {customers_df.columns}\")\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(\"\\nFirst 10 rows:\")\n",
    "    customers_df.show(10)\n",
    "    \n",
    "    # Check schema\n",
    "    print(\"\\n=== Schema Information ===\")\n",
    "    customers_df.printSchema()\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\n=== Column Data Types ===\")\n",
    "    for col_name, col_type in customers_df.dtypes:\n",
    "        print(f\"{col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fccae4",
   "metadata": {},
   "source": [
    "### Task 2.3: Loading JSON Data\n",
    "\n",
    "Now let's work with semi-structured JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Loading JSON Data ===\")\n",
    "    \n",
    "    # Load JSON data (each line is a separate JSON object)\n",
    "    transactions_df = spark.read.json(transactions_json_path)\n",
    "    \n",
    "    print(\"✓ JSON loaded successfully!\")\n",
    "    \n",
    "    # Basic information\n",
    "    print(f\"Number of transactions: {transactions_df.count()}\")\n",
    "    print(f\"Columns: {transactions_df.columns}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nSample transactions:\")\n",
    "    transactions_df.show(5, truncate=False)\n",
    "    \n",
    "    # Check schema - notice how nested JSON is handled\n",
    "    print(\"\\n=== JSON Schema ===\")\n",
    "    transactions_df.printSchema()\n",
    "    \n",
    "    # Notice the nested 'metadata' structure\n",
    "    print(\"\\n=== Accessing Nested Data ===\")\n",
    "    transactions_df.select(\"transaction_id\", \"amount\", \"metadata.device\", \"metadata.browser\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c293cfb8",
   "metadata": {},
   "source": [
    "### Task 2.4: Creating and Saving Parquet Data\n",
    "\n",
    "Let's create a Parquet file from our products data and explore its benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1c66a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Working with Parquet Format ===\")\n",
    "    \n",
    "    # First, create DataFrame from products data\n",
    "    products_df = spark.createDataFrame(products_data)\n",
    "    \n",
    "    print(\"Products DataFrame created:\")\n",
    "    products_df.show(10)\n",
    "    products_df.printSchema()\n",
    "    \n",
    "    # Save as Parquet\n",
    "    products_parquet_path = os.path.join(temp_dir, \"products.parquet\")\n",
    "    products_df.write.mode(\"overwrite\").parquet(products_parquet_path)\n",
    "    \n",
    "    print(f\"✓ Saved as Parquet: {products_parquet_path}\")\n",
    "    \n",
    "    # Load back from Parquet\n",
    "    products_loaded_df = spark.read.parquet(products_parquet_path)\n",
    "    \n",
    "    print(\"\\n✓ Loaded from Parquet successfully!\")\n",
    "    print(\"Schema preserved:\")\n",
    "    products_loaded_df.printSchema()\n",
    "    \n",
    "    # Compare file sizes (approximate)\n",
    "    print(\"\\n=== File Format Comparison ===\")\n",
    "    \n",
    "    # Save same data as CSV for comparison\n",
    "    products_csv_path = os.path.join(temp_dir, \"products_comparison.csv\")\n",
    "    products_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(products_csv_path)\n",
    "    \n",
    "    # Save as JSON for comparison\n",
    "    products_json_path = os.path.join(temp_dir, \"products_comparison.json\")\n",
    "    products_df.coalesce(1).write.mode(\"overwrite\").json(products_json_path)\n",
    "    \n",
    "    print(\"✓ Saved products in multiple formats for comparison\")\n",
    "    print(\"Parquet benefits: columnar storage, compression, schema preservation, faster reads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0d01be",
   "metadata": {},
   "source": [
    "## Part 3: DataFrame Operations\n",
    "\n",
    "Now let's learn essential DataFrame operations for data pipeline development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c43e49e",
   "metadata": {},
   "source": [
    "### Task 3.1: Selection and Projection\n",
    "\n",
    "Learn how to select specific columns and create derived columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e4980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Selection and Projection ===\")\n",
    "    \n",
    "    # Select specific columns\n",
    "    print(\"1. Select specific columns:\")\n",
    "    customers_df.select(\"customer_id\", \"first_name\", \"last_name\", \"email\").show(10)\n",
    "    \n",
    "    # Select with column renaming\n",
    "    print(\"\\n2. Select with column renaming:\")\n",
    "    customers_df.select(\n",
    "        col(\"customer_id\").alias(\"id\"),\n",
    "        concat(col(\"first_name\"), lit(\" \"), col(\"last_name\")).alias(\"full_name\"),\n",
    "        col(\"email\"),\n",
    "        col(\"total_spent\")\n",
    "    ).show(10)\n",
    "    \n",
    "    # Add computed columns\n",
    "    print(\"\\n3. Add computed columns:\")\n",
    "    enriched_customers = customers_df.withColumn(\"full_name\", \n",
    "                                                concat(col(\"first_name\"), lit(\" \"), col(\"last_name\"))) \\\n",
    "                                    .withColumn(\"avg_order_value\", \n",
    "                                               round(col(\"total_spent\") / col(\"total_orders\"), 2)) \\\n",
    "                                    .withColumn(\"customer_segment\", \n",
    "                                               when(col(\"total_spent\") > 1000, \"Premium\")\n",
    "                                               .when(col(\"total_spent\") > 500, \"Gold\")\n",
    "                                               .otherwise(\"Standard\"))\n",
    "    \n",
    "    enriched_customers.select(\"customer_id\", \"full_name\", \"total_spent\", \"avg_order_value\", \"customer_segment\").show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c24fc29",
   "metadata": {},
   "source": [
    "### Task 3.2: Filtering and Conditional Operations\n",
    "\n",
    "Learn how to filter data and apply conditional logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c1bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Filtering and Conditional Operations ===\")\n",
    "    \n",
    "    # Basic filtering\n",
    "    print(\"1. Customers with high spending (>$1000):\")\n",
    "    high_value_customers = customers_df.filter(col(\"total_spent\") > 1000)\n",
    "    high_value_customers.select(\"customer_id\", \"first_name\", \"last_name\", \"total_spent\").show()\n",
    "    \n",
    "    # Multiple conditions\n",
    "    print(f\"\\n2. Young customers (age < 30) in specific cities:\")\n",
    "    young_city_customers = customers_df.filter(\n",
    "        (col(\"age\") < 30) & \n",
    "        (col(\"city\").isin([\"New York\", \"Los Angeles\", \"Chicago\"]))\n",
    "    )\n",
    "    young_city_customers.select(\"customer_id\", \"first_name\", \"age\", \"city\", \"total_spent\").show()\n",
    "    \n",
    "    # Filter transactions data\n",
    "    print(\"\\n3. Completed transactions over $100:\")\n",
    "    high_value_transactions = transactions_df.filter(\n",
    "        (col(\"status\") == \"completed\") & \n",
    "        (col(\"amount\") > 100)\n",
    "    )\n",
    "    high_value_transactions.select(\"transaction_id\", \"customer_id\", \"amount\", \"product_category\").show(10)\n",
    "    \n",
    "    # Complex filtering with nested data\n",
    "    print(\"\\n4. Mobile transactions with credit card:\")\n",
    "    mobile_cc_transactions = transactions_df.filter(\n",
    "        (col(\"metadata.device\") == \"mobile\") & \n",
    "        (col(\"payment_method\") == \"credit_card\")\n",
    "    )\n",
    "    mobile_cc_transactions.select(\"transaction_id\", \"amount\", \"metadata.device\", \"payment_method\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7983fa",
   "metadata": {},
   "source": [
    "### Task 3.3: Data Quality Checks\n",
    "\n",
    "Essential data quality checks for any data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2b886",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Data Quality Checks ===\")\n",
    "    \n",
    "    # 1. Check for null values\n",
    "    print(\"1. Null value analysis for customers:\")\n",
    "    null_counts = customers_df.select([\n",
    "        count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\") \n",
    "        for c in customers_df.columns\n",
    "    ])\n",
    "    null_counts.show()\n",
    "    \n",
    "    # 2. Check for duplicates\n",
    "    print(f\"\\n2. Duplicate analysis:\")\n",
    "    total_customers = customers_df.count()\n",
    "    unique_emails = customers_df.select(\"email\").distinct().count()\n",
    "    unique_customer_ids = customers_df.select(\"customer_id\").distinct().count()\n",
    "    \n",
    "    print(f\"Total customers: {total_customers}\")\n",
    "    print(f\"Unique emails: {unique_emails}\")\n",
    "    print(f\"Unique customer IDs: {unique_customer_ids}\")\n",
    "    print(f\"Email duplicates: {total_customers - unique_emails}\")\n",
    "    print(f\"ID duplicates: {total_customers - unique_customer_ids}\")\n",
    "    \n",
    "    # 3. Data profiling - basic statistics\n",
    "    print(\"\\n3. Basic statistics:\")\n",
    "    customers_df.describe().show()\n",
    "    \n",
    "    # 4. Value distribution analysis\n",
    "    print(\"\\n4. City distribution:\")\n",
    "    customers_df.groupBy(\"city\").count().orderBy(desc(\"count\")).show()\n",
    "    \n",
    "    print(\"\\n5. Age distribution by ranges:\")\n",
    "    customers_df.withColumn(\"age_group\", \n",
    "                           when(col(\"age\") < 25, \"18-24\")\n",
    "                           .when(col(\"age\") < 35, \"25-34\")\n",
    "                           .when(col(\"age\") < 45, \"35-44\")\n",
    "                           .when(col(\"age\") < 55, \"45-54\")\n",
    "                           .otherwise(\"55+\")) \\\n",
    "               .groupBy(\"age_group\").count().orderBy(\"age_group\").show()\n",
    "    \n",
    "    # 5. Transaction status analysis\n",
    "    print(\"\\n6. Transaction status distribution:\")\n",
    "    transactions_df.groupBy(\"status\").count().show()\n",
    "    \n",
    "    # 6. Data completeness report\n",
    "    print(\"\\n7. Data Completeness Report:\")\n",
    "    total_transactions = transactions_df.count()\n",
    "    print(f\"Total transactions: {total_transactions}\")\n",
    "    \n",
    "    completeness_report = transactions_df.select([\n",
    "        (count(col(c)) / total_transactions * 100).alias(f\"{c}_completeness_pct\")\n",
    "        for c in [\"transaction_id\", \"customer_id\", \"amount\", \"timestamp\"]\n",
    "    ])\n",
    "    completeness_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22903d65",
   "metadata": {},
   "source": [
    "### Task 3.4: Grouping and Aggregation\n",
    "\n",
    "Learn essential aggregation operations for data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299d2c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Grouping and Aggregation ===\")\n",
    "    \n",
    "    # 1. Customer summary by city\n",
    "    print(\"1. Customer statistics by city:\")\n",
    "    city_summary = customers_df.groupBy(\"city\").agg(\n",
    "        count(\"*\").alias(\"customer_count\"),\n",
    "        avg(\"age\").alias(\"avg_age\"),\n",
    "        avg(\"total_spent\").alias(\"avg_spent\"),\n",
    "        sum(\"total_spent\").alias(\"total_revenue\"),\n",
    "        max(\"total_spent\").alias(\"max_spent\")\n",
    "    ).orderBy(desc(\"total_revenue\"))\n",
    "    \n",
    "    city_summary.show()\n",
    "    \n",
    "    # 2. Transaction analysis by category\n",
    "    print(\"\\n2. Transaction analysis by product category:\")\n",
    "    category_analysis = transactions_df.filter(col(\"status\") == \"completed\") \\\n",
    "                                      .groupBy(\"product_category\").agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction\"),\n",
    "        min(\"amount\").alias(\"min_transaction\"),\n",
    "        max(\"amount\").alias(\"max_transaction\")\n",
    "    ).orderBy(desc(\"total_sales\"))\n",
    "    \n",
    "    category_analysis.show()\n",
    "    \n",
    "    # 3. Customer segmentation\n",
    "    print(\"\\n3. Customer segmentation analysis:\")\n",
    "    customer_segments = customers_df.withColumn(\"spending_tier\",\n",
    "                                               when(col(\"total_spent\") > 2000, \"High\")\n",
    "                                               .when(col(\"total_spent\") > 1000, \"Medium\")\n",
    "                                               .when(col(\"total_spent\") > 500, \"Low\")\n",
    "                                               .otherwise(\"Minimal\")) \\\n",
    "                                    .groupBy(\"spending_tier\").agg(\n",
    "        count(\"*\").alias(\"customer_count\"),\n",
    "        avg(\"age\").alias(\"avg_age\"),\n",
    "        avg(\"total_orders\").alias(\"avg_orders\"),\n",
    "        sum(\"total_spent\").alias(\"segment_revenue\")\n",
    "    )\n",
    "    \n",
    "    customer_segments.show()\n",
    "    \n",
    "    # 4. Time-based analysis (transactions by payment method) \n",
    "    # Payment method analysis (no window function, no warning)\n",
    "    print(\"\\n4. Payment method analysis:\")\n",
    "\n",
    "    payment_stats = transactions_df.filter(col(\"status\") == \"completed\") \\\n",
    "    .groupBy(\"payment_method\").agg(\n",
    "        count(\"*\").alias(\"usage_count\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\"),\n",
    "        sum(\"amount\").alias(\"total_amount\")\n",
    "    )\n",
    "\n",
    "    # Compute total usage_count (as a scalar)\n",
    "    total_usage = payment_stats.agg(sum(\"usage_count\").alias(\"total_usage\")).collect()[0][\"total_usage\"]\n",
    "\n",
    "    # Add usage_percentage column\n",
    "    payment_analysis = payment_stats.withColumn(\n",
    "        \"usage_percentage\",\n",
    "        round(col(\"usage_count\") / total_usage * 100, 2)\n",
    "    )\n",
    "\n",
    "    payment_analysis.orderBy(desc(\"usage_count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fb3c9",
   "metadata": {},
   "source": [
    "## Part 4: Introduction to Spark SQL\n",
    "\n",
    "Learn how to use SQL syntax with Spark DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f81d59",
   "metadata": {},
   "source": [
    "### Task 4.1: Creating Temporary Views\n",
    "\n",
    "Register DataFrames as temporary SQL tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab440352",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Creating Temporary Views ===\")\n",
    "    \n",
    "    # Register DataFrames as temporary views\n",
    "    customers_df.createOrReplaceTempView(\"customers\")\n",
    "    transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "    products_loaded_df.createOrReplaceTempView(\"products\")\n",
    "    \n",
    "    print(\"✓ Created temporary views:\")\n",
    "    print(\"  - customers\")\n",
    "    print(\"  - transactions\") \n",
    "    print(\"  - products\")\n",
    "    \n",
    "    # List all temporary views\n",
    "    print(f\"\\nAvailable tables: {spark.catalog.listTables()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bfbb6",
   "metadata": {},
   "source": [
    "### Task 4.2: Basic SQL Queries\n",
    "\n",
    "Practice essential SQL operations using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Basic SQL Queries ===\")\n",
    "    \n",
    "    # 1. Simple SELECT with WHERE\n",
    "    print(\"1. High-value customers (SQL approach):\")\n",
    "    high_value_sql = spark.sql(\"\"\"\n",
    "        SELECT customer_id, first_name, last_name, total_spent, city\n",
    "        FROM customers \n",
    "        WHERE total_spent > 1000 \n",
    "        ORDER BY total_spent DESC\n",
    "    \"\"\")\n",
    "    high_value_sql.show(10)\n",
    "    \n",
    "    # 2. Aggregation with GROUP BY\n",
    "    print(\"\\n2. Customer count by city (SQL approach):\")\n",
    "    city_counts_sql = spark.sql(\"\"\"\n",
    "        SELECT city, \n",
    "               COUNT(*) as customer_count,\n",
    "               ROUND(AVG(age), 1) as avg_age,\n",
    "               ROUND(AVG(total_spent), 2) as avg_spent\n",
    "        FROM customers \n",
    "        GROUP BY city \n",
    "        ORDER BY customer_count DESC\n",
    "    \"\"\")\n",
    "    city_counts_sql.show()\n",
    "    \n",
    "    # 3. Working with JSON/nested data in SQL\n",
    "    print(\"\\n3. Transaction analysis with nested data (SQL approach):\")\n",
    "    device_analysis_sql = spark.sql(\"\"\"\n",
    "        SELECT metadata.device as device_type,\n",
    "               COUNT(*) as transaction_count,\n",
    "               ROUND(AVG(amount), 2) as avg_amount,\n",
    "               SUM(amount) as total_amount\n",
    "        FROM transactions \n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY metadata.device \n",
    "        ORDER BY transaction_count DESC\n",
    "    \"\"\")\n",
    "    device_analysis_sql.show()\n",
    "    \n",
    "    # 4. Complex query with multiple conditions\n",
    "    print(\"\\n4. Complex analysis - Premium customers in major cities:\")\n",
    "    premium_analysis_sql = spark.sql(\"\"\"\n",
    "        SELECT city,\n",
    "               COUNT(*) as premium_customer_count,\n",
    "               ROUND(AVG(total_spent), 2) as avg_premium_spent,\n",
    "               ROUND(AVG(age), 1) as avg_age\n",
    "        FROM customers \n",
    "        WHERE total_spent > 1500 \n",
    "        AND city IN ('New York', 'Los Angeles', 'Chicago')\n",
    "        GROUP BY city\n",
    "        ORDER BY premium_customer_count DESC\n",
    "    \"\"\")\n",
    "    premium_analysis_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ccec71",
   "metadata": {},
   "source": [
    "### Task 4.3: DataFrame vs SQL Comparison\n",
    "\n",
    "Compare the same operations using DataFrame API and SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== DataFrame vs SQL Comparison ===\")\n",
    "    \n",
    "    # Same analysis using both approaches\n",
    "    print(\"Analysis: Average transaction amount by product category\")\n",
    "    \n",
    "    # DataFrame approach\n",
    "    print(\"\\n--- DataFrame API Approach ---\")\n",
    "    df_result = transactions_df.filter(col(\"status\") == \"completed\") \\\n",
    "                              .groupBy(\"product_category\") \\\n",
    "                              .agg(\n",
    "                                  count(\"*\").alias(\"transaction_count\"),\n",
    "                                  round(avg(\"amount\"), 2).alias(\"avg_amount\")\n",
    "                              ) \\\n",
    "                              .orderBy(desc(\"avg_amount\"))\n",
    "    \n",
    "    df_result.show()\n",
    "    \n",
    "    # SQL approach\n",
    "    print(\"\\n--- SQL Approach ---\")\n",
    "    sql_result = spark.sql(\"\"\"\n",
    "        SELECT product_category,\n",
    "               COUNT(*) as transaction_count,\n",
    "               ROUND(AVG(amount), 2) as avg_amount\n",
    "        FROM transactions \n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY product_category \n",
    "        ORDER BY avg_amount DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    sql_result.show()\n",
    "    \n",
    "    # Verify results are identical\n",
    "    print(f\"\\nResults identical: {df_result.collect() == sql_result.collect()}\")\n",
    "    \n",
    "    print(\"\\n=== When to use DataFrame vs SQL ===\")\n",
    "    print(\"DataFrame API:\")\n",
    "    print(\"  ✓ Better for programmatic operations\")\n",
    "    print(\"  ✓ Type safety and IDE support\")\n",
    "    print(\"  ✓ Method chaining\")\n",
    "    print(\"  ✓ Integration with Python/Scala code\")\n",
    "    \n",
    "    print(\"\\nSQL:\")\n",
    "    print(\"  ✓ Familiar syntax for SQL users\")\n",
    "    print(\"  ✓ Complex queries can be more readable\")\n",
    "    print(\"  ✓ Easy to port from traditional databases\")\n",
    "    print(\"  ✓ Better for ad-hoc analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c7a6ea",
   "metadata": {},
   "source": [
    "### Task 4.4: Advanced SQL Operations\n",
    "\n",
    "Explore more advanced SQL features in Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df85d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Advanced SQL Operations ===\")\n",
    "    \n",
    "    # 1. Window functions\n",
    "    print(\"1. Customer ranking by spending within each city:\")\n",
    "    customer_ranking_sql = spark.sql(\"\"\"\n",
    "        SELECT customer_id, \n",
    "               first_name, \n",
    "               last_name, \n",
    "               city, \n",
    "               total_spent,\n",
    "               RANK() OVER (PARTITION BY city ORDER BY total_spent DESC) as city_rank,\n",
    "               DENSE_RANK() OVER (ORDER BY total_spent DESC) as overall_rank\n",
    "        FROM customers \n",
    "        WHERE total_spent > 500\n",
    "        ORDER BY city, city_rank\n",
    "    \"\"\")\n",
    "    customer_ranking_sql.show(20)\n",
    "    \n",
    "    # 2. Common Table Expressions (CTE)\n",
    "    print(\"\\n2. Using CTE for complex analysis:\")\n",
    "    cte_analysis_sql = spark.sql(\"\"\"\n",
    "        WITH customer_segments AS (\n",
    "            SELECT customer_id,\n",
    "                   first_name,\n",
    "                   last_name,\n",
    "                   total_spent,\n",
    "                   CASE \n",
    "                       WHEN total_spent > 2000 THEN 'Premium'\n",
    "                       WHEN total_spent > 1000 THEN 'Gold'\n",
    "                       WHEN total_spent > 500 THEN 'Silver'\n",
    "                       ELSE 'Bronze'\n",
    "                   END as segment\n",
    "            FROM customers\n",
    "        ),\n",
    "        segment_stats AS (\n",
    "            SELECT segment,\n",
    "                   COUNT(*) as customer_count,\n",
    "                   ROUND(AVG(total_spent), 2) as avg_spent,\n",
    "                   MIN(total_spent) as min_spent,\n",
    "                   MAX(total_spent) as max_spent\n",
    "            FROM customer_segments\n",
    "            GROUP BY segment\n",
    "        )\n",
    "        SELECT segment,\n",
    "               customer_count,\n",
    "               avg_spent,\n",
    "               min_spent,\n",
    "               max_spent,\n",
    "               ROUND(customer_count * 100.0 / SUM(customer_count) OVER(), 2) as percentage\n",
    "        FROM segment_stats\n",
    "        ORDER BY avg_spent DESC\n",
    "    \"\"\")\n",
    "    cte_analysis_sql.show()\n",
    "    \n",
    "    # 3. Date/time functions\n",
    "    print(\"\\n3. Transaction trends (simulated with random timestamps):\")\n",
    "    time_analysis_sql = spark.sql(\"\"\"\n",
    "        SELECT DATE(timestamp) as transaction_date,\n",
    "               COUNT(*) as daily_transactions,\n",
    "               ROUND(SUM(amount), 2) as daily_revenue,\n",
    "               ROUND(AVG(amount), 2) as avg_transaction_amount\n",
    "        FROM transactions \n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY DATE(timestamp)\n",
    "        ORDER BY transaction_date DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    time_analysis_sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a852ce07",
   "metadata": {},
   "source": [
    "## Lab Summary and Performance Analysis\n",
    "\n",
    "Let's wrap up with a performance comparison of different file formats and approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc457f5",
   "metadata": {},
   "source": [
    "### Task 5.1: File Format Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c6f1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== File Format Performance Comparison ===\")\n",
    "    \n",
    "    # Let's create larger datasets for meaningful performance comparison\n",
    "    print(\"Creating larger datasets for performance testing...\")\n",
    "    \n",
    "    # Create larger customer dataset\n",
    "    large_customers_data = []\n",
    "    for i in range(1, 10001):  # 10k customers\n",
    "        customer = {\n",
    "            'customer_id': i,\n",
    "            'first_name': random.choice(['John', 'Jane', 'Bob', 'Alice', 'Charlie', 'Diana', 'Eve', 'Frank']),\n",
    "            'last_name': random.choice(['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis']),\n",
    "            'email': f'customer{i}@email.com',\n",
    "            'age': random.randint(18, 80),\n",
    "            'city': random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia']),\n",
    "            'signup_date': (datetime.now() - timedelta(days=random.randint(1, 365))).strftime('%Y-%m-%d'),\n",
    "            'total_orders': random.randint(0, 50),\n",
    "            'total_spent': builtins.round(random.uniform(0, 5000), 2)\n",
    "        }\n",
    "        large_customers_data.append(customer)\n",
    "    \n",
    "    large_customers_df = spark.createDataFrame(large_customers_data)\n",
    "    \n",
    "    # Save in different formats\n",
    "    csv_path = os.path.join(temp_dir, \"large_customers.csv\")\n",
    "    json_path = os.path.join(temp_dir, \"large_customers.json\")\n",
    "    parquet_path = os.path.join(temp_dir, \"large_customers.parquet\")\n",
    "    \n",
    "    print(\"Saving in different formats...\")\n",
    "    \n",
    "    # Time CSV write\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    large_customers_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(csv_path)\n",
    "    csv_write_time = time.time() - start_time\n",
    "    \n",
    "    # Time JSON write\n",
    "    start_time = time.time()\n",
    "    large_customers_df.coalesce(1).write.mode(\"overwrite\").json(json_path)\n",
    "    json_write_time = time.time() - start_time\n",
    "    \n",
    "    # Time Parquet write\n",
    "    start_time = time.time()\n",
    "    large_customers_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "    parquet_write_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nWrite Performance:\")\n",
    "    print(f\"CSV write time: {csv_write_time:.3f} seconds\")\n",
    "    print(f\"JSON write time: {json_write_time:.3f} seconds\")\n",
    "    print(f\"Parquet write time: {parquet_write_time:.3f} seconds\")\n",
    "    \n",
    "    # Test read performance\n",
    "    print(f\"\\nRead Performance:\")\n",
    "    \n",
    "    # Time CSV read\n",
    "    start_time = time.time()\n",
    "    csv_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(csv_path)\n",
    "    csv_df.count()  # Force evaluation\n",
    "    csv_read_time = time.time() - start_time\n",
    "    \n",
    "    # Time JSON read\n",
    "    start_time = time.time()\n",
    "    json_df = spark.read.json(json_path)\n",
    "    json_df.count()  # Force evaluation\n",
    "    json_read_time = time.time() - start_time\n",
    "    \n",
    "    # Time Parquet read\n",
    "    start_time = time.time()\n",
    "    parquet_df = spark.read.parquet(parquet_path)\n",
    "    parquet_df.count()  # Force evaluation\n",
    "    parquet_read_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"CSV read time: {csv_read_time:.3f} seconds\")\n",
    "    print(f\"JSON read time: {json_read_time:.3f} seconds\")\n",
    "    print(f\"Parquet read time: {parquet_read_time:.3f} seconds\")\n",
    "    \n",
    "    print(f\"\\n=== Performance Summary ===\")\n",
    "    print(\"Parquet is typically fastest for:\")\n",
    "    print(\"  ✓ Read operations (columnar format)\")\n",
    "    print(\"  ✓ Analytical queries (column pruning)\")\n",
    "    print(\"  ✓ Storage efficiency (compression)\")\n",
    "    print(\"  ✓ Schema preservation\")\n",
    "    \n",
    "    print(\"\\nCSV is good for:\")\n",
    "    print(\"  ✓ Human readability\")\n",
    "    print(\"  ✓ Interoperability with other tools\")\n",
    "    print(\"  ✓ Simple data structures\")\n",
    "    \n",
    "    print(\"\\nJSON is good for:\")\n",
    "    print(\"  ✓ Semi-structured data\")\n",
    "    print(\"  ✓ Nested/complex data structures\")\n",
    "    print(\"  ✓ Schema flexibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21881303",
   "metadata": {},
   "source": [
    "## Lab Reflection and Next Steps\n",
    "\n",
    "### Key Takeaways from Lab 2\n",
    "\n",
    "#### PySpark Fundamentals\n",
    "1. **SparkSession**: The entry point for all Spark functionality\n",
    "2. **DataFrames**: Primary abstraction for structured data processing\n",
    "3. **Schema Management**: Understanding and controlling data types\n",
    "4. **File Formats**: Each has trade-offs for different use cases\n",
    "\n",
    "#### Essential Operations Learned\n",
    "1. **Data Loading**: CSV, JSON, Parquet with proper configuration\n",
    "2. **Data Inspection**: Schema, statistics, quality checks\n",
    "3. **Transformations**: Select, filter, aggregate, window functions\n",
    "4. **SQL Integration**: Seamless mix of DataFrame API and SQL\n",
    "\n",
    "#### Data Quality Patterns\n",
    "1. **Null Value Analysis**: Essential for data pipeline reliability\n",
    "2. **Duplicate Detection**: Critical for data integrity\n",
    "3. **Data Profiling**: Understanding your data distribution\n",
    "4. **Completeness Checks**: Ensuring data quality standards\n",
    "\n",
    "## Next Steps for Project\n",
    "\n",
    "1. **Choose your project dataset** and analyze its characteristics\n",
    "2. **Plan your data quality strategy** based on what you've learned\n",
    "3. **Decide on file formats** for each layer of your medallion architecture\n",
    "4. **Practice with your own data** using the patterns from this lab\n",
    "\n",
    "The skills you've learned form the foundation for building robust data pipelines.!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db54f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if pyspark_available:\n",
    "    spark.stop()\n",
    "    print(\"✓ Spark session stopped\")\n",
    "\n",
    "# Clean up temporary files\n",
    "try:\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"✓ Cleaned up temporary files: {temp_dir}\")\n",
    "except:\n",
    "    print(f\"Note: You may manually delete: {temp_dir}\")\n",
    "\n",
    "print(\"\\n=== Lab 2 Complete! ===\")\n",
    "print(\"\\nDeliverables completed:\")\n",
    "print(\"✓ PySpark environment setup and verification\")\n",
    "print(\"✓ Data loading from multiple file formats (CSV, JSON, Parquet)\")\n",
    "print(\"✓ Schema exploration and data profiling\")\n",
    "print(\"✓ Essential DataFrame operations (select, filter, aggregate)\")\n",
    "print(\"✓ Data quality analysis and validation\")\n",
    "print(\"✓ SQL integration and comparison with DataFrame API\")\n",
    "print(\"✓ Performance analysis of different file formats\")\n",
    "print(\"✓ Foundation skills for Medallion Architecture implementation\")\n",
    "\n",
    "print(\"\\nYou're now ready to:\")\n",
    "print(\"• Load and inspect your project dataset\")\n",
    "print(\"• Implement data quality checks\")\n",
    "print(\"• Design your Bronze → Silver → Gold pipeline\")\n",
    "print(\"• Choose appropriate file formats and optimization strategies\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
