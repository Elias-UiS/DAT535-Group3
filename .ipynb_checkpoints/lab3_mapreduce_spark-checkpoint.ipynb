{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552e537a",
   "metadata": {},
   "source": [
    "# Lab Exercise 3: MapReduce Implementation and Spark Translation\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand MapReduce paradigm through hands-on implementation in Spark operations\n",
    "- Build foundational skills for data transformations needed in Bronze → Silver → Gold layers\n",
    "\n",
    "### Lab Overview\n",
    "This lab bridges the gap between MapReduce concepts with Spark implementation, preparing you for the final Medallion Architecture project.\n",
    "\n",
    "### Timeline\n",
    "- **Part 1**: MapReduce in Spark (40-50 minutes)  \n",
    "- **Part 2**: Project-Relevant Scenarios (20-30 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed096f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Import Required Libraries\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from typing import List, Tuple, Any\n",
    "import builtins\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# For Spark (will install if needed)\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql.functions import *\n",
    "    import pyspark.sql.functions as F\n",
    "    from pyspark.sql.types import *\n",
    "    pyspark_available = True\n",
    "except ImportError:\n",
    "    print(\"PySpark not available. Install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029fe744",
   "metadata": {},
   "source": [
    "## Part 1: MapReduce Implementation in Spark\n",
    "\n",
    "Now let's implement MapReduce in Spark using both RDD and DataFrame approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "if pyspark_available:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MapReduce to Spark Lab\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    print(\"Spark session initialized successfully!\")\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "else:\n",
    "    print(\"Skipping Spark tasks - PySpark not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f32b46",
   "metadata": {},
   "source": [
    "### Task 1.1: Word Count in Spark (RDD vs DataFrame)\n",
    "\n",
    "Implement the classic word count example to understand the MapReduce pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c67438",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    # RDD Approach - Direct MapReduce translation\n",
    "    print(\"=== RDD Approach (Direct MapReduce Translation) ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Test data\n",
    "    sample_text = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"The dog was really lazy\",\n",
    "        \"Brown fox and lazy dog are friends\"\n",
    "    ]\n",
    "\n",
    "    # Create RDD from sample text\n",
    "    text_rdd = spark.sparkContext.parallelize(sample_text)\n",
    "    \n",
    "    # Map phase: split lines into words and create (word, 1) pairs\n",
    "    word_pairs_rdd = text_rdd.flatMap(lambda line: [(word.lower().strip('.,!?\";'), 1) \n",
    "                                                   for word in line.split() \n",
    "                                                   if word.strip('.,!?\";')])\n",
    "    \n",
    "    # Reduce phase: sum counts for each word\n",
    "    word_counts_rdd = word_pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Collect results\n",
    "    rdd_results = dict(word_counts_rdd.collect())\n",
    "    rdd_time = time.time() - start_time\n",
    "    \n",
    "    print(\"RDD Word Count Results:\")\n",
    "    for word, count in sorted(rdd_results.items()):\n",
    "        print(f\"{word}: {count}\")\n",
    "    print(f\"RDD Execution time: {rdd_time:.4f} seconds\")\n",
    "    print()\n",
    "    \n",
    "    # DataFrame Approach - Modern Spark SQL way\n",
    "    print(\"=== DataFrame Approach (Modern Spark SQL) ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create DataFrame\n",
    "    text_df = spark.createDataFrame([(line,) for line in sample_text], [\"text\"])\n",
    "    \n",
    "    # Split text into words and explode into rows\n",
    "    words_df = text_df.select(explode(split(lower(col(\"text\")), \" \")).alias(\"word\"))\n",
    "    \n",
    "    # Clean words and count\n",
    "    word_counts_df = words_df \\\n",
    "        .filter(col(\"word\") != \"\") \\\n",
    "        .withColumn(\"word\", regexp_replace(col(\"word\"), \"[.,!?\\\"';]\", \"\")) \\\n",
    "        .filter(col(\"word\") != \"\") \\\n",
    "        .groupBy(\"word\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"word\")\n",
    "    \n",
    "    df_results = {row['word']: row['count'] for row in word_counts_df.collect()}\n",
    "    df_time = time.time() - start_time\n",
    "    \n",
    "    print(\"DataFrame Word Count Results:\")\n",
    "    word_counts_df.show()\n",
    "    print(f\"DataFrame Execution time: {df_time:.4f} seconds\")\n",
    "    \n",
    "    # Performance comparison\n",
    "    print(f\"\\n=== Performance Comparison RDD vs DataFrame ===\")\n",
    "    print(f\"Spark RDD: {rdd_time:.4f} seconds\") \n",
    "    print(f\"Spark DataFrame: {df_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1e40a1",
   "metadata": {},
   "source": [
    "### Task 1.2: Sales Aggregation in Spark\n",
    "\n",
    "Implement MapReduce for aggregating sales data by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a61e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    # Sample sales data\n",
    "    sales_data = [\n",
    "        {'category': 'Electronics', 'amount': 1200, 'customer': 'John'},\n",
    "        {'category': 'Clothing', 'amount': 300, 'customer': 'Jane'},\n",
    "        {'category': 'Electronics', 'amount': 800, 'customer': 'Bob'},\n",
    "        {'category': 'Books', 'amount': 50, 'customer': 'Alice'},\n",
    "        {'category': 'Clothing', 'amount': 150, 'customer': 'Charlie'},\n",
    "        {'category': 'Electronics', 'amount': 2000, 'customer': 'David'},\n",
    "        {'category': 'Books', 'amount': 75, 'customer': 'Eve'}\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame from sales data\n",
    "    sales_df = spark.createDataFrame(sales_data)\n",
    "    \n",
    "    print(\"=== Original Sales Data ===\")\n",
    "    sales_df.show()\n",
    "    \n",
    "    # RDD Approach\n",
    "    print(\"=== RDD Approach ===\")\n",
    "    sales_rdd = spark.sparkContext.parallelize(sales_data)\n",
    "    \n",
    "    # Map to (category, amount) pairs and aggregate\n",
    "    category_amounts = sales_rdd.map(lambda x: (x['category'], x['amount']))\n",
    "    sales_by_category_rdd = category_amounts.groupByKey().mapValues(list)\n",
    "    \n",
    "    rdd_aggregation = sales_by_category_rdd.map(\n",
    "        lambda x: (x[0], {\n",
    "            'total_sales': builtins.sum(x[1]),\n",
    "            'sales_count': len(x[1]),\n",
    "            'average': builtins.sum(x[1]) / len(x[1])\n",
    "        })\n",
    "    ).collect()\n",
    "    \n",
    "    print(\"RDD Aggregation Results:\")\n",
    "    for category, stats in rdd_aggregation:\n",
    "        print(f\"{category}: Total=${stats['total_sales']}, Count={stats['sales_count']}, Avg=${stats['average']:.2f}\")\n",
    "    \n",
    "    # DataFrame Approach\n",
    "    print(\"\\n=== DataFrame Approach ===\")\n",
    "    sales_summary_df = sales_df.groupBy(\"category\").agg(\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"amount\").alias(\"sales_count\"),\n",
    "        avg(\"amount\").alias(\"average\")\n",
    "    ).orderBy(\"category\")\n",
    "    \n",
    "    sales_summary_df.show()\n",
    "    \n",
    "    window_spec = Window.rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "    \n",
    "    # More advanced DataFrame operations\n",
    "    print(\"=== Advanced DataFrame Analysis ===\")\n",
    "    advanced_summary = sales_df.groupBy(\"category\").agg(\n",
    "        sum(\"amount\").alias(\"total_sales\"),\n",
    "        F.count(\"amount\").alias(\"transaction_count\"),\n",
    "        avg(\"amount\").alias(\"average_sale\"),\n",
    "        min(\"amount\").alias(\"min_sale\"),\n",
    "        max(\"amount\").alias(\"max_sale\"),\n",
    "        stddev(\"amount\").alias(\"stddev_sale\")\n",
    "    ).withColumn(\"sales_percentage\", \n",
    "                 round(col(\"total_sales\") / sum(\"total_sales\").over(window_spec), 3) * 100\n",
    "                ).orderBy(desc(\"total_sales\"))\n",
    "    \n",
    "    advanced_summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318adb47",
   "metadata": {},
   "source": [
    "### Task 1.3: Data Cleaning in Spark\n",
    "\n",
    "Implement Data cleaning and standardizing customer records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8edfb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    # Sample customer data with issues\n",
    "    customer_data = [\n",
    "        {'id': 1, 'name': 'john DOE', 'email': 'JOHN@EMAIL.COM', 'phone': '(555) 123-4567'},\n",
    "        {'id': 2, 'name': 'jane smith', 'email': 'jane@email.com', 'phone': '555-987-6543'},\n",
    "        {'id': 3, 'name': 'bob johnson', 'email': 'invalid_email', 'phone': '555 111 2222'},\n",
    "        {'id': 4, 'name': 'alice brown', 'email': 'alice@email.com', 'phone': '(555)444-3333'},\n",
    "        {'id': 5, 'name': 'john DOE', 'email': 'john@email.com', 'phone': '555-123-4567'}  # Duplicate\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame from customer data\n",
    "    customer_df = spark.createDataFrame(customer_data)\n",
    "    \n",
    "    print(\"=== Original Customer Data ===\")\n",
    "    customer_df.show()\n",
    "    \n",
    "    # DataFrame approach for data cleaning\n",
    "    print(\"=== Cleaned Customer Data (DataFrame Approach) ===\")\n",
    "    \n",
    "    cleaned_df = customer_df \\\n",
    "        .filter(col(\"email\").contains(\"@\")) \\\n",
    "        .withColumn(\"name\", initcap(trim(col(\"name\")))) \\\n",
    "        .withColumn(\"email\", lower(trim(col(\"email\")))) \\\n",
    "        .withColumn(\"phone\", regexp_replace(regexp_replace(regexp_replace(\n",
    "            col(\"phone\"), \"[()-]\", \"\"), \" \", \"\"), \"-\", \"\")) \\\n",
    "        .dropDuplicates([\"email\"]) \\\n",
    "        .withColumn(\"status\", lit(\"cleaned\"))\n",
    "    \n",
    "    cleaned_df.show()\n",
    "    \n",
    "    # Show cleaning statistics\n",
    "    print(\"=== Data Cleaning Statistics ===\")\n",
    "    original_count = customer_df.count()\n",
    "    valid_email_count = customer_df.filter(col(\"email\").contains(\"@\")).count()\n",
    "    final_count = cleaned_df.count()\n",
    "    \n",
    "    print(f\"Original records: {original_count}\")\n",
    "    print(f\"Records with valid email: {valid_email_count}\")\n",
    "    print(f\"Final cleaned records: {final_count}\")\n",
    "    print(f\"Records removed (invalid email): {original_count - valid_email_count}\")\n",
    "    print(f\"Records removed (duplicates): {valid_email_count - final_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66aa5c37",
   "metadata": {},
   "source": [
    "## Part 2: Project-Relevant Scenarios (Medallion Architecture)\n",
    "\n",
    "Now let's apply our MapReduce and Spark knowledge to scenarios that directly relate to the Bronze → Silver → Gold architecture pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfc4a3",
   "metadata": {},
   "source": [
    "### Task 2.1: Bronze Layer Simulation - Raw Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2820bd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate raw JSON data that might come from various sources\n",
    "raw_json_data = [\n",
    "    '{\"timestamp\": \"2024-01-01T10:00:00\", \"user_id\": 123, \"event\": \"login\", \"device\": \"mobile\", \"location\": \"US\"}',\n",
    "    '{\"timestamp\": \"2024-01-01T10:05:00\", \"user_id\": 456, \"event\": \"purchase\", \"amount\": 99.99, \"product\": \"widget\", \"device\": \"desktop\"}',\n",
    "    '{\"timestamp\": \"2024-01-01T10:10:00\", \"user_id\": 123, \"event\": \"view\", \"page\": \"homepage\", \"device\": \"mobile\"}',\n",
    "    '{\"timestamp\": \"2024-01-01T10:15:00\", \"user_id\": 789, \"event\": \"signup\", \"email\": \"user@example.com\", \"device\": \"tablet\"}',\n",
    "    '{\"timestamp\": \"2024-01-01T10:20:00\", \"user_id\": 456, \"event\": \"logout\", \"device\": \"desktop\"}',\n",
    "    # Some malformed data to simulate real-world scenarios\n",
    "    '{\"timestamp\": \"2024-01-01T10:25:00\", \"user_id\": \"invalid\", \"event\": \"error\"}',\n",
    "    '{\"malformed\": \"json\"}'\n",
    "]\n",
    "\n",
    "if pyspark_available:\n",
    "    print(\"=== Bronze Layer: Raw Data Ingestion ===\")\n",
    "    \n",
    "    # Create RDD from raw JSON strings\n",
    "    raw_rdd = spark.sparkContext.parallelize(raw_json_data)\n",
    "    \n",
    "    # Parse JSON and handle errors (Bronze layer pattern)\n",
    "    def parse_json_safe(json_str):\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            data['_ingestion_timestamp'] = time.time()\n",
    "            data['_source'] = 'api'\n",
    "            data['_status'] = 'valid'\n",
    "            return data\n",
    "        except:\n",
    "            return {\n",
    "                '_raw_data': json_str,\n",
    "                '_ingestion_timestamp': time.time(),\n",
    "                '_source': 'api',\n",
    "                '_status': 'parse_error'\n",
    "            }\n",
    "    \n",
    "    # Apply parsing\n",
    "    bronze_rdd = raw_rdd.map(parse_json_safe)\n",
    "    bronze_data = bronze_rdd.collect()\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    bronze_df = spark.createDataFrame(bronze_data)\n",
    "    \n",
    "    print(\"Bronze Layer Data (Raw with Metadata):\")\n",
    "    bronze_df.show(truncate=False)\n",
    "    \n",
    "    # Show data quality metrics\n",
    "    total_records = bronze_df.count()\n",
    "    valid_records = bronze_df.filter(col(\"_status\") == \"valid\").count()\n",
    "    error_records = bronze_df.filter(col(\"_status\") == \"parse_error\").count()\n",
    "    \n",
    "    print(f\"\\nData Quality Metrics:\")\n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"Valid records: {valid_records}\")\n",
    "    print(f\"Parse errors: {error_records}\")\n",
    "    print(f\"Success rate: {(valid_records/total_records)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4530c2",
   "metadata": {},
   "source": [
    "### Task 2.2: Silver Layer Preview - Data Cleaning and Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb1136",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Silver Layer: Cleaned and Standardized Data ===\")\n",
    "    \n",
    "    # Start with valid Bronze layer data\n",
    "    valid_bronze_df = bronze_df.filter(col(\"_status\") == \"valid\")\n",
    "    \n",
    "    # Silver layer transformations\n",
    "    silver_df = valid_bronze_df \\\n",
    "        .withColumn(\"timestamp\", to_timestamp(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"user_id\", col(\"user_id\").cast(\"integer\")) \\\n",
    "        .withColumn(\"event_date\", to_date(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"event_hour\", hour(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"amount\", col(\"amount\").cast(\"double\")) \\\n",
    "        .filter(col(\"user_id\").isNotNull()) \\\n",
    "        .withColumn(\"_silver_processed_timestamp\", current_timestamp()) \\\n",
    "        .drop(\"_ingestion_timestamp\", \"_source\", \"_status\")\n",
    "    \n",
    "    print(\"Silver Layer Data (Cleaned and Typed):\")\n",
    "    silver_df.show()\n",
    "    \n",
    "    # Data validation and quality checks\n",
    "    print(\"=== Silver Layer Data Quality ===\")\n",
    "    \n",
    "    # Check for null values in critical fields\n",
    "    null_checks = silver_df.select([\n",
    "        F.count(when(col(c).isNull(), c)).alias(f\"{c}_nulls\") \n",
    "        for c in [\"user_id\", \"event\", \"timestamp\"]\n",
    "    ])\n",
    "    null_checks.show()\n",
    "    \n",
    "    # Event type distribution\n",
    "    print(\"Event Type Distribution:\")\n",
    "    silver_df.groupBy(\"event\").count().orderBy(desc(\"count\")).show()\n",
    "    \n",
    "    # Device distribution\n",
    "    print(\"Device Distribution:\")\n",
    "    silver_df.groupBy(\"device\").count().orderBy(desc(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4df810",
   "metadata": {},
   "source": [
    "### Task 2.3: Gold Layer Preparation - Business Metrics and Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9c5b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Gold Layer: Business Metrics and Analytics ===\")\n",
    "    \n",
    "    # User activity summary (Gold layer aggregation)\n",
    "    user_activity_gold = silver_df.groupBy(\"user_id\").agg(\n",
    "        F.count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"event\").alias(\"unique_events\"),\n",
    "        min(\"timestamp\").alias(\"first_activity\"),\n",
    "        max(\"timestamp\").alias(\"last_activity\"),\n",
    "        sum(\"amount\").alias(\"total_spent\"),\n",
    "        countDistinct(\"device\").alias(\"devices_used\")\n",
    "    ).withColumn(\"session_duration_minutes\", \n",
    "                 (unix_timestamp(\"last_activity\") - unix_timestamp(\"first_activity\")) / 60\n",
    "    )\n",
    "    \n",
    "    print(\"User Activity Summary (Gold Layer):\")\n",
    "    user_activity_gold.show()\n",
    "    \n",
    "    # Daily metrics rollup\n",
    "    daily_metrics_gold = silver_df.groupBy(\"event_date\").agg(\n",
    "        F.count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        sum(\"amount\").alias(\"daily_revenue\"),\n",
    "        avg(\"amount\").alias(\"avg_transaction\"),\n",
    "        F.count(when(col(\"event\") == \"purchase\", 1)).alias(\"purchases\"),\n",
    "        F.count(when(col(\"event\") == \"login\", 1)).alias(\"logins\"),\n",
    "        F.count(when(col(\"event\") == \"signup\", 1)).alias(\"signups\")\n",
    "    ).withColumn(\"conversion_rate\", \n",
    "                 round(col(\"purchases\") / col(\"unique_users\") * 100, 2)\n",
    "    )\n",
    "    \n",
    "    print(\"Daily Metrics Summary (Gold Layer):\")\n",
    "    daily_metrics_gold.show()\n",
    "    \n",
    "    # Hourly activity pattern\n",
    "    hourly_pattern_gold = silver_df.groupBy(\"event_hour\").agg(\n",
    "        F.count(\"*\").alias(\"events_count\"),\n",
    "        countDistinct(\"user_id\").alias(\"active_users\")\n",
    "    ).orderBy(\"event_hour\")\n",
    "    \n",
    "    print(\"Hourly Activity Pattern (Gold Layer):\")\n",
    "    hourly_pattern_gold.show()\n",
    "    \n",
    "    # Device preference analysis\n",
    "    device_analysis_gold = silver_df.groupBy(\"device\", \"event\").agg(\n",
    "        F.count(\"*\").alias(\"event_count\")\n",
    "    ).groupBy(\"device\").pivot(\"event\").sum(\"event_count\").fillna(0)\n",
    "    \n",
    "    print(\"Device Preference Analysis (Gold Layer):\")\n",
    "    device_analysis_gold.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3982a2",
   "metadata": {},
   "source": [
    "### Task 2.4: Performance Analysis and Optimization Insights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727dbdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Performance Analysis: Spark Optimization Techniques ===\")\n",
    "    \n",
    "    # Compare different approaches for the same operation\n",
    "    \n",
    "    # Approach 1: Multiple passes (inefficient)\n",
    "    print(\"Approach 1: Multiple DataFrame scans (inefficient)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    total_events_1 = silver_df.count()\n",
    "    unique_users_1 = silver_df.select(\"user_id\").distinct().count()\n",
    "    total_revenue_1 = silver_df.agg(sum(\"amount\")).collect()[0][0] or 0\n",
    "    \n",
    "    approach1_time = time.time() - start_time\n",
    "    print(f\"Results: {total_events_1} events, {unique_users_1} users, ${total_revenue_1:.2f} revenue\")\n",
    "    print(f\"Time: {approach1_time:.4f} seconds\")\n",
    "    \n",
    "    # Approach 2: Single pass with aggregation (efficient)\n",
    "    print(\"\\nApproach 2: Single DataFrame scan with aggregation (efficient)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    summary_stats = silver_df.agg(\n",
    "        F.count(\"*\").alias(\"total_events\"),\n",
    "        countDistinct(\"user_id\").alias(\"unique_users\"),\n",
    "        sum(\"amount\").alias(\"total_revenue\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    approach2_time = time.time() - start_time\n",
    "    print(f\"Results: {summary_stats['total_events']} events, {summary_stats['unique_users']} users, ${summary_stats['total_revenue'] or 0:.2f} revenue\")\n",
    "    print(f\"Time: {approach2_time:.4f} seconds\")\n",
    "    \n",
    "    # Approach 3: Cached DataFrame (best for multiple operations)\n",
    "    print(\"\\nApproach 3: Cached DataFrame for multiple operations\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    cached_df = silver_df.cache()\n",
    "    cached_df.count()  # Trigger caching\n",
    "    \n",
    "    total_events_3 = cached_df.count()\n",
    "    unique_users_3 = cached_df.select(\"user_id\").distinct().count()\n",
    "    total_revenue_3 = cached_df.agg(sum(\"amount\")).collect()[0][0] or 0\n",
    "    \n",
    "    approach3_time = time.time() - start_time\n",
    "    print(f\"Results: {total_events_3} events, {unique_users_3} users, ${total_revenue_3:.2f} revenue\")\n",
    "    print(f\"Time: {approach3_time:.4f} seconds (including cache)\")\n",
    "    \n",
    "    cached_df.unpersist()  # Clean up cache\n",
    "    \n",
    "    print(f\"\\n=== Performance Summary ===\")\n",
    "    print(f\"Approach 1 (Multiple scans): {approach1_time:.4f} seconds\")\n",
    "    print(f\"Approach 2 (Single aggregation): {approach2_time:.4f} seconds\")\n",
    "    print(f\"Approach 3 (Cached): {approach3_time:.4f} seconds\")\n",
    "    \n",
    "    print(f\"\\nKey Insights for Medallion Architecture:\")\n",
    "    print(\"- Bronze layer: Focus on reliable ingestion and error handling\")\n",
    "    print(\"- Silver layer: Optimize data types and partitioning\")\n",
    "    print(\"- Gold layer: Use single-pass aggregations and caching for complex metrics\")\n",
    "    print(\"- Always profile your transformations to identify bottlenecks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d442020",
   "metadata": {},
   "source": [
    "## Lab Summary and Reflection\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "#### MapReduce in Spark Benefits\n",
    "1. **RDD Approach**: Direct MapReduce, good for complex logic\n",
    "2. **DataFrame Approach**: SQL-like operations, better optimization\n",
    "3. **Performance**: Built-in optimizations and distributed execution\n",
    "\n",
    "#### Medallion Architecture Patterns\n",
    "1. **Bronze Layer**: Raw data ingestion with error handling and metadata\n",
    "2. **Silver Layer**: Data cleaning, type conversion, and standardization\n",
    "3. **Gold Layer**: Business metrics, aggregations, and analytics-ready data\n",
    "\n",
    "### Next Steps for Final Project\n",
    "\n",
    "1. **Identify your data sources** and ingestion patterns (Bronze layer)\n",
    "2. **Define data quality rules** and cleaning operations (Silver layer)  \n",
    "3. **Specify business metrics** and aggregation requirements (Gold layer)\n",
    "4. **Plan your Spark optimizations** based on data size and complexity\n",
    "\n",
    "The patterns you've learned will directly translate to your Medallion Architecture implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3536eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "if pyspark_available:\n",
    "    spark.stop()\n",
    "    print(\"Spark session stopped.\")\n",
    "\n",
    "print(\"Lab 3 Complete!\")\n",
    "print(\"\\nDeliverables:\")\n",
    "print(\"✓ MapReduce in Spark (RDD and DataFrame approaches)\")\n",
    "print(\"✓ Performance comparisons\")\n",
    "print(\"✓ Medallion Architecture pattern examples\")\n",
    "print(\"✓ Optimization insights for final project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83f7b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
