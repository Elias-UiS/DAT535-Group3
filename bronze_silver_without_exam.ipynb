{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e94541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark is available!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import json\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "# PySpark imports\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import *\n",
    "    from pyspark.sql.types import *\n",
    "    pyspark_available = True\n",
    "    print(\"PySpark is available!\")\n",
    "except ImportError:\n",
    "    print(\"PySpark not found. Please install with: pip install pyspark\")\n",
    "    pyspark_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb1921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 16:14:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SparkSession created successfully!\n",
      "Spark Version: 3.5.0\n",
      "Application Name: PySpark\n",
      "Master: local[*]\n",
      "Default Parallelism: 4\n"
     ]
    }
   ],
   "source": [
    "if pyspark_available:\n",
    "    # Create SparkSession with custom configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PySpark\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce verbose output\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    print(\"✓ SparkSession created successfully!\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "    print(f\"Master: {spark.sparkContext.master}\")\n",
    "    \n",
    "    # Check available cores and memory\n",
    "    print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot proceed without PySpark. Please install PySpark first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca6159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Bronze Layer: Raw Data Ingestion ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 16:14:49 WARN TaskSetManager: Stage 0 contains a task of very large size (1011 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if pyspark_available:\n",
    "    print(\"=== Bronze Layer: Raw Data Ingestion ===\")\n",
    "\n",
    "    with open(\"./data/students_no_scores.json/part-00000-3615aed5-2901-4b90-81af-4caacb7dba9a-c000.json\", \"r\") as f:\n",
    "        lines = f.read().splitlines()\n",
    "\n",
    "    raw_rdd = spark.sparkContext.parallelize(lines)\n",
    "\n",
    "    def parse_json_safe(json_str):\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            data['_ingestion_timestamp'] = time.time()\n",
    "            data['_source'] = 'file'\n",
    "            data['_status'] = 'valid'\n",
    "            return data\n",
    "        except:\n",
    "            return {\n",
    "                '_raw_data': json_str,\n",
    "                '_ingestion_timestamp': time.time(),\n",
    "                '_source': 'file',\n",
    "                '_status': 'parse_error'\n",
    "            }\n",
    "        \n",
    "    bronze_rdd = raw_rdd.map(parse_json_safe)\n",
    "    bronze_data = bronze_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92579306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 16:14:50 WARN TaskSetManager: Stage 1 contains a task of very large size (1011 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Silver Layer: Cleaned and Standardized Data ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 16:14:51 WARN TaskSetManager: Stage 2 contains a task of very large size (1011 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows after deduplication: 8783 (removed 0)\n",
      "Rows after removing invalid student id: 8783 (removed 0)\n",
      "{'Gender': {'Male': 4623, 'Female': 3888}, 'Parental_Involvement': {'Medium': 4581, 'Low': 1641, 'High': 2289}, 'Access_to_Resources': {'Medium': 4014, 'Low': 2097, 'High': 2412}, 'Extracurricular_Activities': {'Yes': 5250, 'No': 3273}, 'Motivation_Level': {'Medium': 4549, 'Low': 2329, 'High': 1645}, 'Internet_Access': {'Yes': 7896, 'No': 615}, 'Family_Income': {'Medium': 3697, 'Low': 3230, 'High': 1584}, 'Teacher_Quality': {'Medium': 4767, 'Low': 771, 'High': 2872}, 'School_Type': {'Private': 2927, 'Public': 5584}, 'Peer_Influence': {'Neutral': 3591, 'Negative': 1665, 'Positive': 3255}, 'Learning_Disabilities': {'No': 7672, 'Yes': 839}, 'Parental_Education_Level': {'Postgraduate': 1577, 'High School': 4417, 'College': 2409}, 'Distance_from_Home': {'Near': 5234, 'Moderate': 2418, 'Far': 770}, 'Student_ID': 910538582.4747808, 'Hours_Studied': 19.51718878329227, 'Attendance': 81.09445031092338, 'Sleep_Hours': 6.849935468731667, 'Previous_Scores': 74.03426023700575, 'Tutoring_Sessions': 1.5292737299073096, 'Physical_Activity': 2.967147717939693}\n",
      "{'Gender': 'Male', 'Parental_Involvement': 'Medium', 'Access_to_Resources': 'Medium', 'Extracurricular_Activities': 'Yes', 'Motivation_Level': 'Medium', 'Internet_Access': 'Yes', 'Family_Income': 'Medium', 'Teacher_Quality': 'Medium', 'School_Type': 'Public', 'Peer_Influence': 'Neutral', 'Learning_Disabilities': 'No', 'Parental_Education_Level': 'High School', 'Distance_from_Home': 'Near', 'Student_ID': 910538582.4747808, 'Hours_Studied': 19.51718878329227, 'Attendance': 81.09445031092338, 'Sleep_Hours': 6.849935468731667, 'Previous_Scores': 74.03426023700575, 'Tutoring_Sessions': 1.5292737299073096, 'Physical_Activity': 2.967147717939693}\n",
      "Rows after cleaning: 8783\n",
      "=== Field Validation Summary ===\n",
      "Attendance                | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Sleep_Hours               | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Motivation_Level          | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "School_Type               | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Peer_Influence            | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Distance_from_Home        | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Physical_Activity         | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Family_Income             | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Teacher_Quality           | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Learning_Disabilities     | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Parental_Education_Level  | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Student_ID                | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Previous_Scores           | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Tutoring_Sessions         | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Access_to_Resources       | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Gender                    | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Hours_Studied             | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Parental_Involvement      | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Extracurricular_Activities | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "Internet_Access           | Total: 8783  | Valid: 8783  | Invalid: 0    \n",
      "=== Silver Layer Data Quality ===\n",
      "\n",
      "Value distribution for Parental_Involvement:\n",
      "Medium: 4853\n",
      "High: 2289\n",
      "Low: 1641\n",
      "\n",
      "Value distribution for Access_to_Resources:\n",
      "Medium: 4274\n",
      "High: 2412\n",
      "Low: 2097\n",
      "\n",
      "Value distribution for Extracurricular_Activities:\n",
      "Yes: 5510\n",
      "No: 3273\n",
      "\n",
      "Value distribution for Motivation_Level:\n",
      "Medium: 4809\n",
      "Low: 2329\n",
      "High: 1645\n",
      "\n",
      "Value distribution for Internet_Access:\n",
      "Yes: 8168\n",
      "No: 615\n",
      "\n",
      "Value distribution for Family_Income:\n",
      "Medium: 3969\n",
      "Low: 3230\n",
      "High: 1584\n",
      "\n",
      "Value distribution for Teacher_Quality:\n",
      "Medium: 5140\n",
      "High: 2872\n",
      "Low: 771\n",
      "\n",
      "Value distribution for School_Type:\n",
      "Public: 5856\n",
      "Private: 2927\n",
      "\n",
      "Value distribution for Peer_Influence:\n",
      "Neutral: 3863\n",
      "Positive: 3255\n",
      "Negative: 1665\n",
      "\n",
      "Value distribution for Learning_Disabilities:\n",
      "No: 7944\n",
      "Yes: 839\n",
      "\n",
      "Value distribution for Parental_Education_Level:\n",
      "High School: 4797\n",
      "College: 2409\n",
      "Postgraduate: 1577\n",
      "\n",
      "Value distribution for Distance_from_Home:\n",
      "Near: 5595\n",
      "Moderate: 2418\n",
      "Far: 770\n",
      "\n",
      "Value distribution for Gender:\n",
      "Male: 4895\n",
      "Female: 3888\n",
      "+-------------------+----------+------------------+--------------------------+-------------+------+-------------+---------------+---------------------+----------------+------------------------+--------------------+--------------+-----------------+---------------+-----------+-----------+----------+---------------+-----------------+--------------------+---------------------------+-------+-------+---------------+\n",
      "|Access_to_Resources|Attendance|Distance_from_Home|Extracurricular_Activities|Family_Income|Gender|Hours_Studied|Internet_Access|Learning_Disabilities|Motivation_Level|Parental_Education_Level|Parental_Involvement|Peer_Influence|Physical_Activity|Previous_Scores|School_Type|Sleep_Hours|Student_ID|Teacher_Quality|Tutoring_Sessions|_ingestion_timestamp|_silver_processed_timestamp|_source|_status|_corrupt_record|\n",
      "+-------------------+----------+------------------+--------------------------+-------------+------+-------------+---------------+---------------------+----------------+------------------------+--------------------+--------------+-----------------+---------------+-----------+-----------+----------+---------------+-----------------+--------------------+---------------------------+-------+-------+---------------+\n",
      "|Medium             |98.0      |Near              |Yes                       |Medium       |Male  |24.0         |Yes            |No                   |Medium          |Postgraduate            |Medium              |Neutral       |4.0              |91.0           |Public     |7.0        |3.0       |Medium         |2.0              |1.7642600912764E9   |1.7642600976460824E9       |file   |valid  |NULL           |\n",
      "|Medium             |89.0      |Moderate          |Yes                       |Medium       |Male  |29.0         |Yes            |No                   |Medium          |High School             |Low                 |Negative      |4.0              |98.0           |Public     |8.0        |4.0       |Medium         |1.0              |1.7642600912764094E9|1.764260097646119E9        |file   |valid  |NULL           |\n",
      "|High               |78.0      |Far               |Yes                       |High         |Male  |25.0         |Yes            |No                   |Medium          |High School             |Low                 |Negative      |2.0              |50.0           |Public     |6.0        |8.0       |High           |1.0              |1.764260091276457E9 |1.7642600976461263E9       |file   |valid  |NULL           |\n",
      "|High               |94.0      |Near              |No                        |Medium       |Male  |17.0         |Yes            |No                   |High            |College                 |Medium              |Neutral       |1.0              |80.0           |Private    |6.0        |9.0       |Low            |0.0              |1.7642600912764697E9|1.7642600976461456E9       |file   |valid  |NULL           |\n",
      "|High               |80.0      |Moderate          |No                        |Medium       |Male  |17.0         |No             |No                   |Medium          |College                 |Low                 |Neutral       |4.0              |88.0           |Private    |8.0        |11.0      |High           |4.0              |1.7642600912764862E9|1.764260097646152E9        |file   |valid  |NULL           |\n",
      "+-------------------+----------+------------------+--------------------------+-------------+------+-------------+---------------+---------------------+----------------+------------------------+--------------------+--------------+-----------------+---------------+-----------+-----------+----------+---------------+-----------------+--------------------+---------------------------+-------+-------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if pyspark_available:\n",
    "    print(\"=== Silver Layer: Cleaned and Standardized Data ===\")\n",
    "    \n",
    "    # Start with valid Bronze layer data\n",
    "    valid_bronze_df = bronze_rdd.filter(lambda row: row['_status'] == 'valid')\n",
    "    total_valid = valid_bronze_df.count()\n",
    "\n",
    "    # Removes duplicated rows\n",
    "    exclude_cols = {\"_ingestion_timestamp\", \"_source\", \"_status\", \"_raw_data\"}\n",
    "\n",
    "    dedup_rdd = valid_bronze_df.map(\n",
    "        lambda r: (\n",
    "            tuple(sorted((k, v) for k, v in r.items() if k not in exclude_cols)),\n",
    "            r\n",
    "        )\n",
    "    ).reduceByKey(lambda a, b: a) \\\n",
    "    .map(lambda kv: kv[1])\n",
    "        \n",
    "    total_dedup = dedup_rdd.count()\n",
    "    print(f\"Rows after deduplication: {total_dedup} (removed {total_valid - total_dedup})\")\n",
    "\n",
    "    # Drop rows with invalid Student_ID\n",
    "    dedup_rdd_2 = dedup_rdd.filter(\n",
    "        lambda r: r.get(\"Student_ID\") is not None and str(r.get(\"Student_ID\")).isdigit()\n",
    "    )\n",
    "    total_after_student_id_removal = dedup_rdd_2.count()\n",
    "    print(f\"Rows after removing invalid student id: {total_dedup} (removed {total_dedup - total_after_student_id_removal})\")\n",
    "\n",
    "    # Numeric fields: \n",
    "    numeric_fields = [\"Student_ID\", \"Hours_Studied\", \"Attendance\", \"Sleep_Hours\", \"Previous_Scores\",\n",
    "                    \"Tutoring_Sessions\", \"Physical_Activity\"]\n",
    "        \n",
    "    # Categorical fields: cast to string, replace missing with 'Unknown'\n",
    "    categorical_fields = [\"Parental_Involvement\", \"Access_to_Resources\", \"Extracurricular_Activities\",\n",
    "                        \"Motivation_Level\", \"Internet_Access\", \"Family_Income\", \"Teacher_Quality\",\n",
    "                        \"School_Type\", \"Peer_Influence\", \"Learning_Disabilities\", \n",
    "                        \"Parental_Education_Level\", \"Distance_from_Home\", \"Gender\"]\n",
    "    \n",
    "    # Define allowed values\n",
    "    allowed_values = {\n",
    "        \"Gender\": {\"Male\", \"Female\"},\n",
    "        \"Parental_Involvement\": {\"Low\", \"Medium\", \"High\"},\n",
    "        \"Access_to_Resources\": {\"Low\", \"Medium\", \"High\"},\n",
    "        \"Extracurricular_Activities\": {\"Yes\", \"No\"},\n",
    "        \"Motivation_Level\": {\"Low\", \"Medium\", \"High\"},\n",
    "        \"Internet_Access\": {\"Yes\", \"No\"},\n",
    "        \"Family_Income\": {\"Low\", \"Medium\", \"High\"},\n",
    "        \"Teacher_Quality\": {\"Low\", \"Medium\", \"High\"},\n",
    "        \"School_Type\": {\"Public\", \"Private\"},\n",
    "        \"Peer_Influence\": {\"Positive\", \"Neutral\", \"Negative\"},\n",
    "        \"Learning_Disabilities\": {\"Yes\", \"No\"},\n",
    "        \"Parental_Education_Level\": {\"High School\", \"College\", \"Postgraduate\"},\n",
    "        \"Distance_from_Home\": {\"Near\", \"Moderate\", \"Far\"}\n",
    "    }\n",
    "\n",
    "    # Get a dictonary with the default value for each field\n",
    "    field_summary = {}\n",
    "\n",
    "    # Categorical counts\n",
    "    for field, allowed_values2 in allowed_values.items():\n",
    "        counts = (\n",
    "            dedup_rdd_2\n",
    "            .map(lambda r: r.get(field))\n",
    "            .filter(lambda v: v in allowed_values2)  # only allowed values\n",
    "            .map(lambda v: (v, 1))\n",
    "            .reduceByKey(lambda a, b: a + b)\n",
    "            .collectAsMap()\n",
    "        )\n",
    "        field_summary[field] = counts\n",
    "\n",
    "    for field in numeric_fields:\n",
    "        values_rdd = dedup_rdd_2 \\\n",
    "            .map(lambda r: r.get(field)) \\\n",
    "            .filter(lambda v: v not in [None, \"\"]) \\\n",
    "            .map(lambda v: float(v))\n",
    "        \n",
    "        # Compute sum and count\n",
    "        sum_val = values_rdd.sum()\n",
    "        count_val = values_rdd.count()\n",
    "\n",
    "        if count_val > 0:\n",
    "            avg = sum_val / count_val\n",
    "        else:\n",
    "            avg = None\n",
    "        \n",
    "        field_summary[field] = avg\n",
    "\n",
    "\n",
    "    print(field_summary)\n",
    "\n",
    "    default_field_values = {}\n",
    "\n",
    "    for field, value in field_summary.items():\n",
    "        if isinstance(value, dict):\n",
    "            highest_count = -1\n",
    "            dominant_val = None\n",
    "            for category_field, val in value.items():\n",
    "                if val > highest_count:\n",
    "                    highest_count = val\n",
    "                    dominant_val = category_field\n",
    "            default_field_values[field] = dominant_val\n",
    "        else:  # numeric\n",
    "            default_field_values[field] = value\n",
    "\n",
    "    print(default_field_values)\n",
    "\n",
    "\n",
    "    # Initialize counters: each field maps to [total, valid, invalid]\n",
    "    field_stats = {f: [0,0,0] for f in numeric_fields + categorical_fields}\n",
    "    \n",
    "    \n",
    "    # Silver layer transformations\n",
    "    # Cleaning function\n",
    "    def clean_and_cast(row):\n",
    "        row_dict = dict(row)\n",
    "\n",
    "        # Numeric fields\n",
    "        for field in numeric_fields:\n",
    "            val = row_dict.get(field)\n",
    "            if val in [None, \"\"]:\n",
    "                row_dict[field] = default_field_values.get(field)\n",
    "            else:\n",
    "                try:\n",
    "                    val = float(val)\n",
    "                    # Validate ranges for numeric fields, that has an allowed range\n",
    "                    if field in [\"Attendance\",\"Previous_Scores\"] and not (0 <= val <= 100):\n",
    "                        row_dict[field] = default_field_values.get(field)\n",
    "                    else:\n",
    "                        row_dict[field] = val\n",
    "                except:\n",
    "                    row_dict[field] = default_field_values.get(field)\n",
    "\n",
    "        # Categorical fields\n",
    "        for field in categorical_fields:\n",
    "            val = row_dict.get(field)\n",
    "            if val in [None, \"\"]:\n",
    "                row_dict[field] = default_field_values.get(field)\n",
    "            else:\n",
    "                val_str = str(val).strip().title()\n",
    "                if val_str in allowed_values.get(field, set()):\n",
    "                    row_dict[field] = val_str\n",
    "                else:\n",
    "                    row_dict[field] = default_field_values.get(field)\n",
    "\n",
    "        # Add Silver processing timestamp\n",
    "        row_dict[\"_silver_processed_timestamp\"] = time.time()\n",
    "\n",
    "        return row_dict\n",
    "    \n",
    "\n",
    "    # Apply cleaning + type casting\n",
    "    cleaned_rdd = dedup_rdd_2.map(clean_and_cast)\n",
    "\n",
    "    def safe_debug(row):\n",
    "        try:\n",
    "            clean_and_cast(row)\n",
    "            return 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error row: {row}, {e}\")\n",
    "            return 0\n",
    "\n",
    "    cleaned_rdd.map(safe_debug).take(10)\n",
    "\n",
    "    total_cleaned = cleaned_rdd.count()\n",
    "    print(f\"Rows after cleaning: {total_cleaned}\")\n",
    "    \n",
    "    # Collect results\n",
    "    cleaned_data = cleaned_rdd.collect()\n",
    "\n",
    "    # --- Compute field validation stats ---\n",
    "    def row_field_stats(row):\n",
    "        stats = []\n",
    "        for f in numeric_fields:\n",
    "            val = row.get(f)\n",
    "            total = 1\n",
    "            valid = 1 if val not in [None, \"\"] else 0\n",
    "            stats.append((f, (total, valid, 1-valid)))\n",
    "\n",
    "        for f in categorical_fields:\n",
    "            val = row.get(f)\n",
    "            total = 1\n",
    "            valid = 1 if val not in [None, \"\"] else 0\n",
    "            stats.append((f, (total, valid, 1-valid)))\n",
    "        return stats\n",
    "    \n",
    "    stats_rdd = cleaned_rdd.flatMap(row_field_stats)\n",
    "    field_summary = stats_rdd.reduceByKey(lambda a,b: (a[0]+b[0], a[1]+b[1], a[2]+b[2]))\n",
    "\n",
    "    # Print summary\n",
    "    print(\"=== Field Validation Summary ===\")\n",
    "    for field, (total, valid, invalid) in field_summary.collect():\n",
    "        print(f\"{field:<25} | Total: {total:<5} | Valid: {valid:<5} | Invalid: {invalid:<5}\")\n",
    "\n",
    "    # Data validation and quality checks\n",
    "    print(\"=== Silver Layer Data Quality ===\")\n",
    "    \n",
    "    # Check for null values in critical fields\n",
    "    critical_fields = [\"Student_ID\"]\n",
    "    \n",
    "    # Count nulls per field\n",
    "    null_counts = {}\n",
    "    for field in critical_fields:\n",
    "        null_counts[field] = cleaned_rdd.filter(lambda r: r.get(field) in [None, \"\"]).count()\n",
    "\n",
    "\n",
    "    event_counts = cleaned_rdd.map(lambda r: (r.get(\"event\", \"Unknown\"), 1)) \\\n",
    "                            .reduceByKey(lambda a, b: a + b) \\\n",
    "                            .sortBy(lambda x: -x[1])  # sort descending by count\n",
    "\n",
    "\n",
    "    # Function to count values for a single field\n",
    "    def value_counts(rdd, field):\n",
    "        return (rdd\n",
    "                .map(lambda row: (row.get(field, \"Unknown\"), 1))  # get value, default \"Unknown\"\n",
    "                .reduceByKey(lambda a, b: a + b)                 # sum counts\n",
    "                .sortBy(lambda x: -x[1]))                        # sort descending\n",
    "\n",
    "    # Iterate over all categorical fields\n",
    "    for field in categorical_fields:\n",
    "        counts = value_counts(cleaned_rdd, field)\n",
    "        print(f\"\\nValue distribution for {field}:\")\n",
    "        for val, cnt in counts.collect():\n",
    "            print(f\"{val}: {cnt}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Convert Silver RDD to DataFrame for downstream use\n",
    "    silver_df = spark.createDataFrame(cleaned_data)\n",
    "    silver_df.show(5, truncate=False)\n",
    "\n",
    "    silver_df.write.mode(\"overwrite\").parquet(\"./data/silverResultForStudentsWithoutExam\")\n",
    "    spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
