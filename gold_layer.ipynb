{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb1921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark is available!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "\n",
    "\n",
    "# PySpark imports\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    pyspark_available = True\n",
    "    print(\"PySpark is available!\")\n",
    "except ImportError:\n",
    "    print(\"PySpark not found. Please install with: pip install pyspark\")\n",
    "    pyspark_available = False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf77f8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/27 16:25:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SparkSession created successfully!\n",
      "Spark Version: 3.5.0\n",
      "Application Name: PySpark\n",
      "Master: local[*]\n",
      "Default Parallelism: 4\n"
     ]
    }
   ],
   "source": [
    "if pyspark_available:\n",
    "    # Create SparkSession with custom configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PySpark\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    # Set log level to reduce verbose output\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "    \n",
    "    print(\"✓ SparkSession created successfully!\")\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "    print(f\"Master: {spark.sparkContext.master}\")\n",
    "    \n",
    "    # Check available cores and memory\n",
    "    print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Cannot proceed without PySpark. Please install PySpark first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43674bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation for the numerical fields\n",
      "Hours_Studied: 0.4418645699008704\n",
      "Attendance: 0.5817189471046391\n",
      "Sleep_Hours: -0.017758159802785355\n",
      "Previous_Scores: 0.17227893475096628\n",
      "Tutoring_Sessions: 0.15402576160955206\n",
      "Physical_Activity: 0.025237375089293388\n",
      "\n",
      "Variance of fields:\n",
      "Attendance: 0.338\n",
      "Hours_Studied: 0.195\n",
      "Previous_Scores: 0.030\n",
      "Access_to_Resources: 0.028\n",
      "Parental_Involvement: 0.025\n",
      "Tutoring_Sessions: 0.024\n",
      "Parental_Education_Level: 0.011\n",
      "Peer_Influence: 0.010\n",
      "Family_Income: 0.009\n",
      "Motivation_Level: 0.008\n",
      "Distance_from_Home: 0.008\n",
      "Learning_Disabilities: 0.007\n",
      "Teacher_Quality: 0.006\n",
      "Extracurricular_Activities: 0.004\n",
      "Internet_Access: 0.003\n",
      "Physical_Activity: 0.001\n",
      "Sleep_Hours: 0.000\n",
      "School_Type: 0.000\n",
      "Gender: 0.000\n",
      "+--------------------+--------+\n",
      "|             Feature|Variance|\n",
      "+--------------------+--------+\n",
      "|          Attendance|   0.338|\n",
      "|       Hours_Studied|   0.195|\n",
      "|     Previous_Scores|    0.03|\n",
      "| Access_to_Resources|   0.028|\n",
      "|Parental_Involvement|   0.025|\n",
      "|   Tutoring_Sessions|   0.024|\n",
      "|Parental_Educatio...|   0.011|\n",
      "|      Peer_Influence|    0.01|\n",
      "|       Family_Income|   0.009|\n",
      "|    Motivation_Level|   0.008|\n",
      "|  Distance_from_Home|   0.008|\n",
      "|Learning_Disabili...|   0.007|\n",
      "|     Teacher_Quality|   0.006|\n",
      "|Extracurricular_A...|   0.004|\n",
      "|     Internet_Access|   0.003|\n",
      "|   Physical_Activity|   0.001|\n",
      "|         Sleep_Hours|     0.0|\n",
      "|         School_Type|     0.0|\n",
      "|              Gender|     0.0|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 16:26:00 WARN Instrumentation: [88ceae76] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/27 16:26:00 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/27 16:26:01 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients for the numerical fields\n",
      "Hours_Studied: 0.29206437145547837\n",
      "Attendance: 0.19874336767816414\n",
      "Sleep_Hours: -0.029951364368375175\n",
      "Previous_Scores: 0.047634604855014126\n",
      "Tutoring_Sessions: 0.46894348479899794\n",
      "Physical_Activity: 0.12917150729624682\n",
      "Intercept: 41.07062426373279\n",
      "+----------+-----------------+\n",
      "|Exam_Score|       prediction|\n",
      "+----------+-----------------+\n",
      "|      65.0|65.97730584928655|\n",
      "|      62.0|60.61761918684618|\n",
      "|      63.0|64.41548958884157|\n",
      "|      61.0|60.13833170843973|\n",
      "|      64.0|66.48930298079254|\n",
      "|      61.0|61.32130055919798|\n",
      "|      63.0|63.09438335629221|\n",
      "|      63.0| 63.8381955877407|\n",
      "|      63.0|61.77880786151544|\n",
      "|      66.0|65.40692208923832|\n",
      "+----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "RMSE: 2.447822507503709\n",
      "Number of rows: 6378\n",
      "Number of partitions: 4\n",
      "+----------+--------------------+\n",
      "|Student_ID|Predicted_Exam_Score|\n",
      "+----------+--------------------+\n",
      "|    7774.0|   68.65705301605867|\n",
      "|    7776.0|   67.63975379803064|\n",
      "|    7786.0|   71.90422263450395|\n",
      "|    7787.0|   68.03700076843651|\n",
      "|    7792.0|   64.94254578500215|\n",
      "|    7794.0|   66.76067078388667|\n",
      "|    7797.0|    69.1529707872912|\n",
      "|    7798.0|    62.8034418337779|\n",
      "|    7814.0|   67.42312118489411|\n",
      "|    7821.0|   64.67729751736846|\n",
      "+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "\n",
    "cleaned_data = spark.read.parquet(\"./data/silver/silverResult\")\n",
    "\n",
    "\n",
    "# Use only numeric features for now\n",
    "numeric_fields = [\"Hours_Studied\", \"Attendance\", \"Sleep_Hours\",\n",
    "                  \"Previous_Scores\", \"Tutoring_Sessions\", \"Physical_Activity\"]\n",
    "\n",
    "\n",
    "\n",
    "# Categorical fields: cast to string, replace missing with 'Unknown'\n",
    "categorical_fields = [\"Parental_Involvement\", \"Access_to_Resources\", \"Extracurricular_Activities\",\n",
    "                    \"Motivation_Level\", \"Internet_Access\", \"Family_Income\", \"Teacher_Quality\",\n",
    "                    \"School_Type\", \"Peer_Influence\", \"Learning_Disabilities\", \n",
    "                    \"Parental_Education_Level\", \"Distance_from_Home\", \"Gender\"]\n",
    "\n",
    "cleaned_data = cleaned_data.dropna(subset=numeric_fields + categorical_fields + [\"Exam_Score\"])\n",
    "\n",
    "# Ensure numeric fields are double\n",
    "for num in numeric_fields + [\"Exam_Score\"]:\n",
    "    cleaned_data = cleaned_data.withColumn(num, col(num).cast(\"double\"))\n",
    "\n",
    "correlations = {}\n",
    "\n",
    "for colname in numeric_fields:\n",
    "    corr_value = cleaned_data.stat.corr(colname, \"Exam_Score\")\n",
    "    correlations[colname] = corr_value\n",
    "\n",
    "print(\"Correlation for the numerical fields\")\n",
    "for k, v in correlations.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"\")\n",
    "# Calculate eta\n",
    "#  ANOVA  Eta-Squared\n",
    "pdf = cleaned_data.select(categorical_fields + [\"Exam_Score\"]).toPandas()\n",
    "\n",
    "eta_squared_results = {}\n",
    "\n",
    "for cat in categorical_fields:\n",
    "    groups = [group[\"Exam_Score\"].values for _, group in pdf.groupby(cat)]\n",
    "    \n",
    "    # Calculate eta squared = SS_between / SS_total\n",
    "    overall_mean = pdf[\"Exam_Score\"].mean()\n",
    "    ss_total = ((pdf[\"Exam_Score\"] - overall_mean) ** 2).sum()\n",
    "    ss_between = sum(len(g) * (g.mean() - overall_mean)**2 for g in groups)\n",
    "    eta2 = ss_between / ss_total\n",
    "    eta_squared_results[cat] = round(eta2, 3)\n",
    "\n",
    "combined_variance = {}\n",
    "\n",
    "# Numeric: r²\n",
    "for k, v in correlations.items():\n",
    "    combined_variance[k] = v**2\n",
    "\n",
    "# Categorical: eta²\n",
    "for k, v in eta_squared_results.items():\n",
    "    combined_variance[k] = v\n",
    "\n",
    "\n",
    "sorted_variance = dict(sorted(combined_variance.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# eta² and r², are not identical values, but are roughly the same, so we can compare\n",
    "print(\"Variance of fields:\")  \n",
    "for feature, var_exp in sorted_variance.items():\n",
    "    print(f\"{feature}: {var_exp:.3f}\")\n",
    "\n",
    "rows = [Row(Feature=k, Variance=round(float(v), 3)) for k, v in sorted_variance.items()]\n",
    "variance_df = spark.createDataFrame(rows)\n",
    "variance_df.show()\n",
    "\n",
    "\n",
    "########################## Prediction #######################\n",
    "assembler = VectorAssembler(inputCols=numeric_fields, outputCol=\"features\")\n",
    "data = assembler.transform(cleaned_data).select(\"features\", \"Exam_Score\")\n",
    "\n",
    "# Split dataset\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)\n",
    "train_data = train_data.limit(6000) \n",
    "\n",
    "# Train lightweight model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Exam_Score\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "print(\"Coefficients for the numerical fields\")\n",
    "# Show feature importances (coefficients)\n",
    "for feature, coef in zip(numeric_fields, model.coefficients):\n",
    "    print(f\"{feature}: {coef}\")\n",
    "\n",
    "print(f\"Intercept: {model.intercept}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "predictions.select(\"Exam_Score\", \"prediction\").show(10)\n",
    "\n",
    "# Evaluate\n",
    "evaluator = RegressionEvaluator(labelCol=\"Exam_Score\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"RMSE: {rmse}\")\n",
    "\n",
    "\n",
    "print(f\"Number of rows: {data.count()}\")\n",
    "print(f\"Number of partitions: {data.rdd.getNumPartitions()}\")\n",
    "\n",
    "cleaned_data_without_exam = spark.read.parquet(\"./data/silver/silverResultForStudentsWithoutExam\")\n",
    "cleaned_data_without_exam = assembler.transform(cleaned_data_without_exam)\n",
    "\n",
    "predicted_df = model.transform(cleaned_data_without_exam) \\\n",
    "                    .withColumnRenamed(\"prediction\", \"Predicted_Exam_Score\")\n",
    "predicted_df.select(\"Student_ID\", \"Predicted_Exam_Score\").show(10)\n",
    "\n",
    "# Storing student info and their predicted exam score\n",
    "predicted_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./data/gold/gold_student_predictions\")\n",
    "\n",
    "# Storing variance of each category field i.e sleep_hours\n",
    "variance_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"./data/gold/gold_feature_variance\")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
